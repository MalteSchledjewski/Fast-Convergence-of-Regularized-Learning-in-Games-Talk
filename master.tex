\documentclass{beamer}
\usepackage[british]{babel}        % for german language
\usepackage[utf8]{inputenc}        % for umlauts and other non 7bit ascii things
\usepackage[T1]{fontenc}           % this is needed for correct output of umlauts in pdf
\usepackage{lmodern}               % use a vector based font, not a bitmap based font for T1
\usepackage[stretch=10]{microtype} % improves font placements
\usepackage[autostyle=true,german=quotes]{csquotes}

%\usepackage[backend=biber,style=numeric]{biblatex}
%\addbibresource{master.bib}

\usepackage{mathtools}
\usepackage{amsfonts,amsmath,amssymb,amsthm}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\usepackage{ marvosym }
%\usepackage{array, tabularx}
\usepackage{colortbl}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{backcolour}{rgb}{0.8,0.8,0.8}
\usepackage{graphicx}
\usepackage{subfigure}
\hypersetup{pdfstartview={Fit}}
\usepackage{xparse}
\usepackage{soul}
\usepackage{xcolor}

\newcommand{\E}{\mathbb{E}}
\newcommand{\abs}[1]{\vert{#1}\vert}
\renewcommand{\vec}[1]{\ensuremath{{\bf #1}}}
\newcommand{\dotp}[2]{\ensuremath{\left\langle #1, #2 \right\rangle}}
\newcommand{\strat}{\ensuremath{s}}
\newcommand{\mst}{\ensuremath{w}}
\newcommand{\opt}{\text{\textsc{Opt}} }
\newcommand{\myprop}{\ensuremath{\texttt{RVU}}}
\newcommand{\mR}{\ensuremath{\mathcal{R}}}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\newcommand{\R}{\ensuremath{\mathbb R}}


\newtheorem{proposition}[theorem]{Proposition}

\setbeamercolor{postit}{fg=black,bg=yellow}


\newcommand{\inserteqstrut}[1]{%
	\rlap{$\displaystyle#1$}%
	\phantom{\biggesteq}}


\newcommand{\biggesteq}{%
	\vphantom{\sum_{i=1}^{d} w_i log(w_i)}% tallest/deepest
	\hphantom{\sum_{i=1}^{d} w_i log(w_i)}% longest/widest
}


\makeatletter
\NewDocumentCommand{\sotwo}{O{red}O{black}+m}
{%
	\begingroup
	\color{#1}%
	\setul{-.5ex}{.4pt}%
	\def\SOUL@uleverysyllable{%
		\rlap{%
			\color{#2}\the\SOUL@syllable
			\SOUL@setkern\SOUL@charkern}%
		\SOUL@ulunderline{%
			\phantom{\the\SOUL@syllable}}%
	}%
	\ul{#3}%
	\endgroup
}
\makeatother


\title{Fast Convergence of Regularized Learning in Games}
\author{Malte Schledjewski}
\institute{Saarbr√ºcken Graduate School of Computer Science}
\date{2016}

\usetheme{Frankfurt} %Darmstadt

\begin{document}
	
\frame{\titlepage}

\begin{frame}
	\frametitle{The paper}
	\begin{block}{Fast Convergence of Regularized Learning in Games}
		\begin{itemize}
			\item Vasilis Syrgkanis, Microsoft Research
			\item Alekh Agarwal, Microsoft Research
			\item Haipeng Luo, Princeton University
			\item Robert E. Schapire, Microsoft Research
		\end{itemize}
		published 2015
	\end{block}
\end{frame}


\section{Online learning}
\subsection{Models}
\begin{frame}[c]
	\begin{center}
		\Huge Online learning
	\end{center}
\end{frame}


\begin{frame}
	\frametitle{Online learning}
	\begin{block}{General model}
		You try to map inputs $ x \in \mathcal{X}$ to outputs $y \in \mathcal{Y}$.\\  \pause
		For each time step t: \pause
		\begin{itemize}
			\item You receive $x_t$. \pause 
			\item You predict the output as $p_t$. \pause
			\item You receive the correct output $y_t$. \pause
			\item You suffer some loss $\ell(y_t,p_t)$. \pause
			\item You update your model. \pause
		\end{itemize}
		
		Your goal is minimal accumulated loss.
	\end{block}
\end{frame}

\begin{frame}
	\frametitle{Online learning}
	\begin{block}{Prediction with Expert Advice}
		Consider $d$ experts giving you advices.\\
		Choose the best advice.\\ 
		
		For each time step t:\pause
		\begin{itemize}
			\item You receive $x_t$, a vector of $d$ advices. \pause 
			\item You chose expert $p_t$ and follow his advice. \pause
			\item You receive $y_t$, the vector of costs for following each of the advices. \pause
			\item You suffer some loss $\ell(y_t,p_t) = y_{t,p_t}$. \pause
			\item You update your model.
		\end{itemize}
	\end{block}
\end{frame}

\subsection{Regret}
\begin{frame}
	\frametitle{Regret}

	How do you know how good you are?
	\pause
	
	Compare yourself to the experts.
	\pause
	
	\vspace{1em}
	In each round there is an expert with minimal loss so far.\\
	This is the \textcolor{blue}{leading expert}.
	\pause
	\begin{block}{Regret}
		$r(T)$ := (your cumulated loss) - ( the leader's cumulated loss)
	\end{block}

	\pause
	\begin{block}{No-regret algorithm}
		A no-regret algorithm always achieves regret that is sublinear in T.
	\end{block}

\end{frame}

\subsection{Follow the Leader}
\begin{frame}
	\frametitle{Follow the Leader}
	
	\begin{block}{Follow the Leader}
		Always trust the currently leading expert with his advice for the next round. 
	\end{block}
	
	\pause
	
	\begin{block}{Worst case regret is not sublinear}
		Example:\\
		Binary classification: $y \in \{A, B\}$\\
		Two experts: one always predicts A, the other one always B\\
		Your loss is 0 if you predict right or 1 if you predict wrong.\\ \pause
		\vspace{1em}
		In the worst case your prediction is always false.\\
		Your regret is at least $T/2$.
	\end{block}	
\end{frame}

\subsection{Randomness}
\begin{frame}
	\frametitle{Deterministic or not?}
	
	\begin{block}{An adversaries perspective}
		Finite amount of experts and deterministic behaviour allow easy construction of worst case scenario.\\
		Always make the algorithm's prediction false.
	\end{block}	
	 \pause
	\begin{block}{Idea: randomness}
		Instead of picking one expert just give the probabilities of choosing the experts.\\
		The adversary is not allowed to know the draw.\\
		We then try to minimize accumulated expected loss.
	\end{block}	
\end{frame}


\subsection{Stability}
\begin{frame}
	\frametitle{Stability is also important}
		\begin{block}{Follow the Leader -- regret bound by cheating}
			Let $f_1 ,\ldots, f_T$ be the sequence of loss functions and \\
			$w_1 ,\ldots, w_T$ be the probabilities determined by \textit{Follow the Leader},
			and $w^*$ the leading probabilities.
			
			\begin{equation*}
				r(T) =  \sum_{t=1}^{T}  \left( f_t(w_t) - f_t(w^*) \right)
				\leq
				\sum_{t=1}^{T} \left( \underbrace{f_t(w_t) - f_t(w_{t+1})}_\text{ \onslide<2->{\textcolor{red}{stability}}} \right)
			\end{equation*}
		\end{block}

\pause[3]
	\begin{block}{Follow the Regularized Leader \only<4>{\textcolor{red}{with entropic regularizer}}}
	\begin{equation*}
	w_T
	= \argmin_{w \in \Delta}  \left(\sum_{t=1}^{T-1} f_t(w) \right) + \frac{1}{\eta}  \only<3>{\inserteqstrut{\mR(w)}} \only<4>{\inserteqstrut{\sum_{i=1}^{d} w_i log(w_i)}}
	\end{equation*}
	\end{block}

\end{frame}


\subsection{Online learning summary}
\begin{frame}
	\frametitle{Online learning summary}
	\begin{itemize}
		\item Expert Advice Framework \pause
		\item Regret \pause
		\item Randomness + stability \pause
	\end{itemize}
	
	\begin{block}{Follow the Regularized Leader}
		\textit{Follow the Regularized Leader} is a no-regret algorithm with $r(T)\in O(\sqrt(T))$.
	\end{block}
\end{frame}




\section{Games}
\begin{frame}[c]
	\begin{center}
		\Huge Games
	\end{center}
\end{frame}

\subsection{Matching pennies}
\begin{frame}[c]
		\frametitle{Matching pennies}
	\begin{center}
		
		
		\begin{tabular}{c||c|c}
			$A\diagdown B$ & Heads & Tails \\ \hline \hline
			Heads & $1\diagdown -1$ & $-1\diagdown 1$ \\ \hline
			Tails & $-1\diagdown 1$ & $1\diagdown -1$
		\end{tabular} 
	\end{center}
		\pause
		
		\vspace{1em}
		Each player has to chose one of the possible strategies $S = \left\lbrace \text{Heads}, \text{Tails} \right\rbrace$. \\ \pause
		In each round the players \sotwo[red]{suffer loss} gain utility.\\ \pause
		\vspace{1em}
		Each player wants to maximize his accumulated utility.
\end{frame}



\subsection{Game model}
\begin{frame}
	\frametitle{Game}
	For a game G of $n$ players:\\
	Each player $i$ has 
	\begin{itemize}
		\item a finite strategy space $S_i$ and a
		\item utility function $u_i : S_1 \times \ldots \times S_n \rightarrow [0,1]$.
	\end{itemize}\pause
	\vspace{1em}
	In each round t:\\
	The player chooses a \sotwo[red]{strategy} mixed strategy $\vec{w}^t_i$ to play. \\ \pause
	Then the player receives $u^t$, the expected utility for each of his strategies $x$: $\mathbf{u}^t_i = (u^t_{i,x})_{x \in S_i} $ with $ u^t_{i,x} = \mathbb{E}_{s_{-i} \sim w^t_{-i}} \left[u_i(x,s_{-i})\right] $ \pause \\
	The expected utility for a player $i$ in iteration $t$ is therefore  $ \left\langle w^t_{i},u^t_{i}\right\rangle $.
	
\end{frame}
  % theoretical model
  % played against adversarial environment





\section{Playing under nice conditions}
\begin{frame}[c]
	\begin{center}
		\Huge Playing under nice conditions
	\end{center}
\end{frame}
\subsection{Nice opponents}
\begin{frame}
	\frametitle{Nice opponents}
%	\begin{block}{}
%
%	\end{block}
	Assume all players to use no-regret algorithms.\\ \pause
	\vspace{1em} 
	For two-player zero-sum games each player's average regret converges at the rate of $O(1/T)$ instead of $O(1/\sqrt{T})$.\\ \pause
	\vspace{1em}
	\begin{beamercolorbox}[sep=1em]{postit}
		Can this be generalized?
	\end{beamercolorbox}
	
\end{frame}


\subsection{\myprop~ property}
\begin{frame}
	\frametitle{\myprop~ property}
	\begin{block}{\myprop~ -- Regret bounded by Variation in Utilities}
		A vanishing regret algorithm has the \myprop~ property with parameters $\alpha>0$ and $0<\beta\leq\gamma$  if for any sequence of utilities $\vec{u}^1, \vec{u}^2, \ldots, \vec{u}^T$ the regret is bounded as 
		\begin{equation*}
		r(T) \leq \alpha
		+\beta \sum_{t=1}^{T} \max_{x \in S_i} \left| u_{i,x}^t - u_{i,x}^{t-1} \right|_1^2 -
		\gamma\sum_{t=1}^{T} \left\|\vec{w}^{t} - \vec{w}^{t-1}\right\|^2_1
		\end{equation*}  
	\end{block}
	

\end{frame}

  \subsection{All players' average regret}
  \begin{frame}
  	\frametitle{All players' average regret}
	
	\begin{block}{Fast convergence of all players' average regret}
		Suppose that the algorithm of each player $i$ satisfies the \myprop~property
		with parameters $\alpha, \beta$ and $\gamma$ such that \textcolor{olive}{
		$\beta\leq \gamma/(n-1)^2$}. \\
		Then $\sum_{i\in N} r_i(T) \leq \alpha n$ and therefore all players' average regret converges at a rate of $O(1/T)$.
	\end{block}
	\pause

    	\begin{align*}
    	\onslide<2->{\sum_{i\in N} r_i(T) &\leq 
    	\alert<3>{\sum_{i\in N}} \left( \alpha
    	+\beta \alert<3>{\sum_{t=1}^{T}} \max_{x \in S_i} \left| u_{i,x}^t - u_{i,x}^{t-1} \right|^2 -
    	\gamma\alert<3>{\sum_{t=1}^{T}} \left\|\vec{w}^{t}_i - \vec{w}^{t-1}_i\right\|^2_1 \right)\\}
    	\onslide<3->{&=\alpha n + \sum_{t=1}^{T} \left( \beta \alert<4>{\sum_{i\in N}  \max_{x \in S_i} \left| u_{i,x}^t - u_{i,x}^{t-1} \right|^2}
    	- \gamma \alt<4>{\textcolor{blue}{\sum_{i\in N} \left\|\vec{w}^{t}_i - \vec{w}^{t-1}_i\right\|^2_1}}{\textcolor{black}{\sum_{i\in N} \left\|\vec{w}^{t} - \vec{w}^{t-1}\right\|^2_1}}  \right)}
    	\end{align*}


  \end{frame}

\begin{frame}
	\frametitle{Proof - intermediate step}
	
	\begin{align*}
		&\textcolor{red}{\max_{x \in S_i} \left| u_{i,x}^t - u_{i,x}^{t-1} \right|}\\
	   \onslide<2->{=&\max_{x \in S_i} \left| \E_{\vec{s}_{-i}\sim \vec{w}_{-i}^t}[u_i(x,\vec{s}_{-i})] - \E_{\vec{s}_{-i}\sim \vec{w}_{-i}^{t-1}} [u_i(x,\vec{s}_{-i})] \right| \\}
	   \onslide<3->{=&\max_{x \in S_i} \left| \sum_{\tilde{s} \in \vec{s}_{-i}} u_i(x,\tilde{s}) \left( \text{Prob}^t(\tilde{s}) - \text{Prob}^{t-1}(\tilde{s})\right)   \right|\\}
	   \onslide<4->{\leq&\max_{x \in S_i} \sum_{\tilde{s} \in \vec{s}_{-i}} u_i(x,\tilde{s})  \left| ( \text{Prob}^t(\tilde{s}) - \text{Prob}^{t-1}(\tilde{s}))   \right|\\}
	   \onslide<5->{\leq& \sum_{\tilde{s} \in \vec{s}_{-i}} \left| \prod_{j\neq i} \mst_{j,\tilde{s}_j}^t - \prod_{j\neq i}\mst_{j,\tilde{s}_j}^{t-1}\right| }
	   \onslide<6->{\leq \sum_{j\neq i} \|
	   \vec{\mst}_j^t-\vec{\mst}_j^{t-1}\|}\\
		\onslide<7->{\Rightarrow &\textcolor{red}{\sum_{i\in N} \max_{x \in S_i} \left| u_{i,x}^t - u_{i,x}^{t-1} \right|^2}
		\leq (n-1)^2 \textcolor{blue}{\sum_{i\in N} \left\| \vec{\mst}_i^t-
		\vec{\mst}_i^{t-1}\right\|^2_1}}
	\end{align*}
\end{frame}

\begin{frame}
	\frametitle{Proof - continued}
	
	\begin{align*}
	\onslide<1->{\sum_{i\in N} r_i(T) &\leq 
		\alpha n + \sum_{t=1}^{T} \left( \beta \alert<1>{\sum_{i\in N}  \max_{x \in S_i} \left| u_{i,x}^t - u_{i,x}^{t-1} \right|^2}
		- \gamma \alt<1>{\textcolor{blue}{\sum_{i\in N} \left\|\vec{w}^{t} - \vec{w}^{t-1}\right\|^2_1} }{\textcolor{black}{\sum_{i\in N} \left\|\vec{w}^{t} - \vec{w}^{t-1}\right\|^2_1}} \right)\\}
	\onslide<2->{
		=&\alpha n + \sum_{t=1}^{T} \left( \beta (n-1)^2 \alt<2>{\textcolor{blue}{\sum_{i\in N} \left\|\vec{w}^{t} - \vec{w}^{t-1}\right\|^2_1}}{\textcolor{black}{\sum_{i\in N} \left\|\vec{w}^{t} - \vec{w}^{t-1}\right\|^2_1}}
		- \gamma \alt<2>{\textcolor{blue}{\sum_{i\in N} \left\|\vec{w}^{t} - \vec{w}^{t-1}\right\|^2_1}}{\textcolor{black}{\sum_{i\in N} \left\|\vec{w}^{t} - \vec{w}^{t-1}\right\|^2_1}}  \right)\\}
	\onslide<3->{
		=&\alpha n + \sum_{t=1}^{T} \left( \underbrace{\left(\beta (n-1)^2 - \gamma\right)}_{\leq 0} \alt<3>{\textcolor{blue}{\sum_{i\in N} \left\|\vec{w}^{t} - \vec{w}^{t-1}\right\|^2_1} }{\sum_{i\in N} \left\|\vec{w}^{t} - \vec{w}^{t-1}\right\|^2_1} \right)\\
		\leq& \alpha n}
	\end{align*}
\end{frame}


\subsection{Optimistic Follow the Regularized Leader}

\begin{frame}
	\frametitle{Optimistic Follow the Regularized Leader}
	\begin{block}{Optimistic Follow the Regularized Leader}
		Let $\mR$ be a suitable regularizer and\\
		$\vec{M}_i^T$ be an adaptive prediction sequence:
	\begin{equation*}
	\vec{\mst}_i^T
	= \argmax_{\vec{\mst} \in \Delta(S_i)} \dotp{\vec{\mst}}{\left(\sum_{t=1}^{T-1} \vec{u}_i^t\right)
		+ \vec{M}_i^T}-\frac{\mR(\vec{\mst})}{\eta}.
	\end{equation*}
	\end{block}
	\pause
	
	\begin{block}{Recency bias}
		\textit{Optimistic Follow the Regularized Leader} has the \myprop~ property with\\
		\begin{itemize}
			\item one-step recency bias $\vec{M}_i^t = \vec{u}_i^{t-1}$
			\item $H$-step recency bias $\vec{M}_i^t = \sum_{\tau = t-H}^{t-1} \vec{u}_i^\tau/H$
			\item geometrically discounted recency bias $\vec{M}_i^t
			= \frac{1}{\sum_{\tau = 0}^{t-1} \delta^{-\tau}} \sum_{\tau =
				0}^{t-1} \delta^{-\tau} \vec{u}_i^\tau$
		\end{itemize}
		
		
	\end{block}
	
\end{frame}

\begin{frame}
		
		\begin{block}{One-step recency bias}
			With $\vec{M}_i^t = \vec{u}_i^{t-1}$ and using stepsize $\eta$, \textit{Optimistic Follow the Regularized Leader} satisfies the \myprop~ property 
			with constants
			$\alpha = R/\eta$, $\beta = \eta$ and $\gamma
			= 1/(4\eta)$\\
			where $R = \max_i \left(\sup_{\vec{f}\in \Delta(S_i)} \mR(\vec{f})-\inf_{\vec{f}\in \Delta(S_i)}\mR(\vec{f})\right)$.
			
		\end{block}
\end{frame}

\subsection{Other contributions}
\begin{frame}
 	\frametitle{Other contributions}
 	
 	
 	\begin{block}{Meta-algorithm}
 		They show a meta-algorithm that uses any \myprop~satisfying algorithm and uses it so that
 		the \myprop~property is preserved but also against adversarial environments the worst case rate is $O(1/\sqrt{T})$.
 	\end{block}
 	\pause
 	
 	\begin{block}{Fast convergence of each player's average regret}
 		If either all players use \\
 		\begin{itemize}
  		\item \textit{Optimistic Follow the Regularized Leader} with $\vec{M}_i^t = \vec{u}_i^{t-1}$ or
 			\item all use the meta-algorithm with the same input algorithm
 		\end{itemize}
 		then each player's average regret converges at the rate of $ O(T^{-3/4})$.
 	\end{block}
 	
\end{frame}







% social welfare
% smooth games


\section{Experimental validation}
\begin{frame}[c]
	\begin{center}
		\Huge Experimental validation
	\end{center}
\end{frame}
\subsection{The experiment}
\begin{frame}
	\frametitle{Results I}
	\begin{figure}[!t]
		\centering
		\subfigure{
			\includegraphics[width=0.45\textwidth,height=0.3\textwidth]{sum_regret.pdf}
		}
		\quad
		\subfigure{
			\includegraphics[width=0.45\textwidth,height=0.3\textwidth]{max_regret.pdf}
		}
		\caption{Maximum and sum of individual regrets over time under the
			Hedge (blue) and \mbox{Optimistic Hedge} (red) dynamics.}\label{fig:regrets}
	\end{figure}
\end{frame}

\begin{frame}
	\frametitle{Results II}
	\begin{figure}[!t]
		\centering
		\subfigure{
			\includegraphics[width=0.45\textwidth,height=0.3\textwidth]{expected_bid.pdf}
		}
		\quad
		\subfigure{
			\includegraphics[width=0.45\textwidth,height=0.3\textwidth]{utilities.pdf}
		}
		\caption{Expected bid and per-iteration utility of a player on one of
			the four items over time, under Hedge (blue) and {Optimistic Hedge}
			(red) dynamics.}\label{fig:bids}
	\end{figure}
\end{frame}




\section{Discussion}
\subsection{Discussion}
\begin{frame}
	\frametitle{Results}
		\begin{itemize}
			\item When all players use no-regret algorithms with \myprop~property: All players' average regret converges at rate $O(1/T)$ instead of $O(1/\sqrt{T})$.
			\item Stability and recency bias are key ingredients for fast converging algorithms, for which \textit{Optimistic Follow the Regularized Leader} is an example.
			\item Every no-regret algorithm with \myprop~property can be used by a meta-algorithm that than also satisfies the \myprop~property and guarantees a worst case rate of $O(1/\sqrt{T})$.
			\item When all players use the same algorithm chosen from OFRL with $\vec{M}_i^t = \vec{u}_i^{t-1}$ or the meta-algorithm with the same input algorithm: Each player's individual regret converges at rate $O(T^{-3/4})$ instead of $O(1/\sqrt{T})$.
		\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Discussion}
		\begin{itemize}
			\item Is \myprop~ necessary? (probably not)
			\item Is observing only the other's players moves instead of the expected utilities also enough to get faster rates?
			\item A precise quantification of the desired behaviour, which is necessary for stable trajectories for $\vec{\mst}_i$, is of great interest.
		\end{itemize}
\end{frame}



\section{}



\begin{frame}
	
\end{frame}
















%\subsection{Finite set of strategies}
%\begin{frame}
%	\frametitle{Finite amount of strategies}
%	For a game G of $n$ players:\\
%	Each player $i$ has a finite strategy space $S_i$.
%	
%\pause
%	\begin{beamercolorbox}[sep=1em]{postit}
%		All combination to bid up to 20 for the four items.
%	\end{beamercolorbox}
%	\pause
%	Utility function $u_i : S_1 \times \ldots \times S_n \rightarrow [0,1]$
%	\pause
%	\begin{beamercolorbox}[sep=1em]{postit}
%		(value - cost) / 20
%	\end{beamercolorbox}
%\end{frame}
%
%
%
%\begin{frame}
%	\frametitle{You just made a friend\ldots}
%	\begin{columns}[T]
%		\begin{column}{.5\textwidth}
%			\includegraphics[height=7cm]{adversary} \pause
%		\end{column}
%		\begin{column}{.5\textwidth}
%			\begin{beamercolorbox}[sep=1em]{postit}
%				He is mean and simply gives the items to the other three guys.
%			\end{beamercolorbox}\pause
%			The adversary can choose the worst outcome for you.
%			
%			\vspace{1em}
%			\pause
%			So just be a little bit random and don't let the adversary know the result of the random draw.
%		\end{column}
%	\end{columns}
%	
%\end{frame}
%
%\subsection{One round}
%
%\begin{frame}
%	\frametitle{Repeat and repeat and repeat and\ldots}
%	\begin{block}{Mixed strategies}
%	\begin{itemize}
%		
%		\item $ w = (w_1 , \ldots, w_n)$, a profile of mixed strategies with
%		\begin{itemize}
%			\item $w_i \in \Delta(S_i)$
%			\item $w_{i,x}$ is the probability of strategy $x \in S_i$
%		\end{itemize}
%		%\item  $U_i(w) = \mathbb{E}_{s\sim{}w}[u_i(s)]$, the expected utility of player $i$
%		
%	\end{itemize}
%	\end{block}
%	\pause
%	\begin{block}{Repetition is good for learning}
%		The game G is repeatedly played for T time steps.\\
%		For each time step:
%		\begin{itemize}
%			\item Each player $i$ picks a mixed strategy $\mathbf{w}^t_i \in \Delta(S_i)$ 
%			\item Each player observes the expected utility for each strategy $x$: $\mathbf{u}^t_i = (u^t_{i,x})_{x \in S_i} $ with $ u^t_{i,x} = \mathbb{E}_{s_{-i} \sim w^t_{-i}} \left[u_i(x,s_{-i})\right] $
%		\end{itemize}
%		
%		The expected utility for a player $i$ in iteration $t$ is therefore 
%		$ \left\langle w^t_{i},u^t_{i}\right\rangle $.
%	\end{block}
%\end{frame}
%
%
%\subsection{Regret}
%
%\begin{frame}
%	\frametitle{Regret or no regret -- that is the question}
%		\begin{block}{Regret}
%			\begin{equation*}
%			r_i(T) = \sup\limits_{\mathbf{w}^*_i \in \Delta (S_i)} \sum_{t=1}^{T} \left< \mathbf{w}^*_i - \mathbf{w}^t_i, \mathbf{u}^t_i \right>
%			\end{equation*}\pause
%			\begin{beamercolorbox}[sep=1em]{postit}
%				How much could you have gained if you had played the best (up to now) possible mixed strategy? 
%			\end{beamercolorbox}
%		\end{block}
%		\pause
%				\begin{block}{No-regret}
%					\begin{equation*}
%					r_i(T) \in o(T)
%					\end{equation*}\pause
%					\begin{beamercolorbox}[sep=1em]{postit}
%						So your average regret converges to 0.
%					\end{beamercolorbox}
%				\end{block}
%\end{frame}
%
%
%\subsection{No regret}
%\begin{frame}
%	\frametitle{No-regret suits you}
%	Assume all players chose $\mathbf{w}^t_i$ based on a no-regret learning algorithm.
%	
%	Suitable because:
%	\begin{itemize}
%		\item regret-bounds hold against adversarial environments
%		\item each player's utility approaches optimality
%		\item when played against each other
%		\begin{itemize}
%			\item the social welfare reaches an approximate optimum
%			\item the mixed strategies converge to an equilibrium based with rates based on the regret bounds
%		\end{itemize} 
%		\item there are well-known families of no-regret algorithms, for which the average regret vanishes at the worst-case rate of $O(1/\sqrt{T})$, which is unimprovable against fully adversarial environments.
%	\end{itemize}
%\end{frame}
%
%
%\subsection{Follow the white rabbit -- oh, I meant the leader}
%\begin{frame}
%	\frametitle{Follow the \st{white rabbit} Leader}
%	\begin{block}{Follow the Leader}
%		Because you regret one mixed strategy the most, just play it in the next round.
%	\end{block}
%	\pause
%	\begin{beamercolorbox}[sep=1em]{postit}
%		What could go wrong?
%	\end{beamercolorbox}
%	\pause
%	\begin{block}{Bad example}
%		\begin{align*}
%			 f(T) =
%			\begin{cases}
%			-0.5       & \quad \text{if } t = 1 \\
%			1  & \quad \text{if } t \text{ is even}\\
%			-1  & \quad \text{if } t > 1 \text{ and } t \text{ is odd}\\
%			\end{cases}
%		\end{align*}
%		\begin{beamercolorbox}[sep=1em]{postit}
%			FTL predicts exactly the opposite.\\
%			Regret compared to constant 0 prediction grows linear.
%		\end{beamercolorbox}
%	\end{block}
%\end{frame}
%
%
%\subsection{You'd better be stable}
%\begin{frame}
%	\frametitle{You'd better be stable}
%	\begin{block}{Follow the Leader -- regret bound}
%		\begin{equation*}
%			r_i(T) =  \sum_{t=1}^{T} \left< \mathbf{w}^*_i - \mathbf{w}^t_i, \mathbf{u}^t_i \right> \leq
%			\sum_{t=1}^{T} \left< \mathbf{w}^{t+1}_i - \mathbf{w}^{t}_i, \mathbf{u}^t_i \right>
%		\end{equation*}
%	\end{block}
%	\pause
%	\begin{beamercolorbox}[sep=1em]{postit}
%		Stable algorithm do not change $\mathbf{w}^t_i$ that much.
%	\end{beamercolorbox}
%	\pause
%	\begin{block}{Follow the Regularized Leader }
%	\begin{equation*}
%	\vec{\mst}_i^T
%	= \argmax_{\vec{\mst} \in \Delta(S_i)} \dotp{\vec{\mst}}{\sum_{t=1}^{T-1} \vec{u}_i^t} \pause-\mR(\vec{\mst})
%	\end{equation*}
%	\end{block}
%\end{frame}
%
%
%
%\subsection{Regularize}
%\begin{frame}
%	\frametitle{Regularize}
%	\begin{block}{Entropic regularization}
%		\begin{equation*}
%		\mR(\vec{\mst}) = \sum_{i=1}^{d} \vec{\mst}_i log(\vec{\mst}_i)
%		\end{equation*}
%	\end{block}
%%	\pause
%%	\begin{beamercolorbox}[sep=1em]{postit}
%%		Stable algorithm do not change $\mathbf{w}^t_i$ that much.
%%	\end{beamercolorbox}
%	\pause
%	\begin{block}{Convexity }
%		Entropic regularization over the probability simplex ($ \left\lbrace \mst \in \R^d | \quad \| \mst\| = 1 \right\rbrace $) is 1-strongly convex.
%	\end{block}
%	\pause
%	\begin{block}{Bound}
%		For FRL regret is in $O(\sqrt{T})$.
%	\end{block}
%\end{frame}
%
%
%\subsection{Smoothness}
%\begin{frame}
%	\frametitle{Smooth games}
%	\begin{block}{Smooth game}
%		A game is
%		$(\lambda,\mu)$-smooth if there exists a strategy profile
%		$\vec{\strat}^*$ such that for any strategy profile $\vec{\strat}$:
%		
%		\begin{equation*}
%		\sum_{i\in N} u_i(\strat_i^*,\vec{\strat}_{-i}) \geq \lambda \opt - \mu
%		W(\vec{\strat})
%		\end{equation*}
%	\end{block}
%		\begin{beamercolorbox}[sep=1em]{postit}
%			Your best mixed strategy is good no matter what others play.
%		\end{beamercolorbox}
%	\pause
%	\begin{block}{}
%		In a $(\lambda,\mu)$-smooth game, if each player $i$ suffers regret at
%		most $r_i(T)$, then:
%		\begin{equation*}
%		\frac{1}{T} \sum_{t=1}^{T}
%		W(\vec{\mst}^t) \geq \frac{\lambda}{1+\mu} \opt
%		- \frac{1}{1+\mu}\frac{1}{T}\sum_{i\in N} r_i(T)
%		\end{equation*}
%	\end{block}
%\end{frame}
%

%
%\begin{frame}[c]
%	\begin{center}
%		\Huge Imagine all the other players were nicely behaving no-regret algorithms.\\\pause
%		Can we reduce our average regret faster?
%	\end{center}
%\end{frame}
%
%\section{Contributions}
%\subsection{Contributions}
%\begin{frame}
%	\frametitle{Contributions}
%	For a certain class of no-regret dynamics in smooth games, when played against each other:
%	\begin{itemize}
%		\item\alert<3>{ The average welfare converges to approximate optimum at the rate $O(1/T)$ instead of $O(1/\sqrt{T})$.}
%		%\item The average welfare is at least $ (\lambda / (1 + \mu)) \text{OPT} - O(1/T) $.
%		\item Each player's average regret converges to zero at rate $O(T^{-3/4})$.
%	\end{itemize}\pause
%	A black-box reduction that preserves the rate in favourable environments and maintains $\tilde{O}(1/\sqrt{T})$ against adversarial environments.
%\end{frame}
%
%
%
%


%  
%  \begin{frame}
%  	\frametitle{Proof result}
%  	\begin{itemize}
%  		\item Average regret vanishes at the rate of $O(1/T)$.
%  		
%  		\item For a smooth game the social welfare reaches the approximate optimum.
%  		
%  	\end{itemize}
%  	
%  \end{frame}
%  
%\section[OFRL]{Recency bias} % a lot
%
%\begin{frame}
%	\frametitle{Optimistic Follow the Regularized Leader}
%	\begin{block}{Optimistic Follow the Regularized Leader}
%		Let $\mR$ be a 1-strongly convex regularizer and\\
%		$\vec{M}_i^T$ be an adaptive prediction sequence:
%	\begin{equation*}
%	\vec{\mst}_i^T
%	= \argmax_{\vec{\mst} \in \Delta(S_i)} \dotp{\vec{\mst}}{\sum_{t=1}^{T-1} \vec{u}_i^t
%		+ \vec{M}_i^T}-\frac{\mR(\vec{\mst})}{\eta}.
%	\end{equation*}
%	\end{block}
%	\pause
%	\begin{block}{One-step recency bias}
%		With $\vec{M}_i^t = \vec{u}_i^{t-1}$ and using stepsize $\eta$, OFRL satisfies the \myprop~ property 
%		 with constants
%			$\alpha = R/\eta$, $\beta = \eta$ and $\gamma
%			= 1/(4\eta)$\\
%		where $R = \max_i \left(\sup_{\vec{f}\in \Delta(S_i)} \mR(\vec{f})-\inf_{\vec{f}\in \Delta(S_i)}\mR(\vec{f})\right)$.
%		
%	\end{block}
%	
%\end{frame}
%


%






% backup slides

\begin{frame}
	
\end{frame}

%TODO: add FTL unstable example to backup slides (notes 154)


%
%\section{Experiment}
%% fast convergence, stable and better equilibrium (higher welfare)
%\subsection{Experimental setting}
%\begin{frame}
%	\frametitle{The experiment}
%	\begin{itemize}
%		\item 4 bidders
%		\item 4 items each worth 20
%		\item each bidder has no extra value from more than one item
%		\item bids are integers in [1,20]
%		\item in every round each bidder only bids for one item 
%		\item highest bid wins and must be paid
%		\item ties are broken arbitrarily 
%	\end{itemize}
%\end{frame}
%

\end{document}
